{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhXFKW0mFKF5",
        "outputId": "951ae71b-8167-410f-b514-9628f6e5fad2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PR7avadCd1p"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Example domains and sub-domains\n",
        "domains = {\n",
        "    \"Medical\": ['Orthopedics', 'Oncology', 'Neurology', 'Pathology', 'Radiology',\n",
        "                'Infectious Diseases', 'Dermatology', 'General Surgery',\n",
        "                'Nephrology', 'Emergency Medicine', 'Plastic Surgery',\n",
        "                'Pediatrics', 'Endocrinology', 'Internal Medicine', 'Cardiology',\n",
        "                'Reproductive Medicine', 'Anesthesiology', 'Hematology']\n",
        "\n",
        "}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "\n",
        "def generate_description(domain, sub_domain):\n",
        "    prompt = f\"Write a detailed 1000-word description about the {sub_domain} in the {domain} domain.\"\n",
        "\n",
        "    try:\n",
        "        # Tokenize the input prompt\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        # Generate multiple outputs to ensure diversity (using Top-p or Top-k with num_return_sequences)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=2000,  # Adjust max_length as needed\n",
        "            num_return_sequences=3,  # Generate 3 different responses\n",
        "            temperature=0.8,  # Introduce randomness\n",
        "            top_k=50,  # Use Top-k sampling for diversity\n",
        "            top_p=0.9,  # Use Top-p (nucleus) sampling for diversity\n",
        "            repetition_penalty=2.0,  # Avoid repetition\n",
        "            do_sample=True,  # Enables sampling (necessary for diversity)\n",
        "        )\n",
        "\n",
        "        # Decode the outputs to text\n",
        "        descriptions = [tokenizer.decode(\n",
        "            output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "        # We can further process to choose the most unique description\n",
        "        unique_description = descriptions[0]\n",
        "        return unique_description\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating description: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Create the dataset\n",
        "def generate_dataset(file_name, rows=10000):\n",
        "    with open(file_name, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Write header\n",
        "        writer.writerow([\"description\", \"domain\", \"sub_domain\"])\n",
        "\n",
        "        for _ in range(rows):\n",
        "            # Randomly select domain and sub-domain\n",
        "            domain = random.choice(list(domains.keys()))\n",
        "            sub_domain = random.choice(domains[domain])\n",
        "\n",
        "            # Generate description\n",
        "            description = generate_description(domain, sub_domain)\n",
        "\n",
        "            # Write row\n",
        "            writer.writerow([description, domain, sub_domain])\n",
        "\n",
        "    print(f\"Dataset with {rows} rows created: {file_name}\")\n",
        "\n",
        "# Generate dataset\n",
        "# generate_dataset(\"domain_classifier_dataset.csv\", rows=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH5KIgrrHoCF"
      },
      "outputs": [],
      "source": [
        "generate_dataset(\"../Dataset/raw data/domain_classifier_augmented_dataset.csv\", rows=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxn5jJZx26Me"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
